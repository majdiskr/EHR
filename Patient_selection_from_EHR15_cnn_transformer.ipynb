{"cells":[{"cell_type":"markdown","metadata":{"id":"C6KELUxZLmYw"},"source":["# Patient Selection"]},{"cell_type":"markdown","metadata":{"id":"eC1se6XbLmY9"},"source":["# 1. Import Libraries and Load Data\n","\n","## 1-1. Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1476,"status":"ok","timestamp":1731146364008,"user":{"displayName":"Majdi Sukkar","userId":"11732602087870485821"},"user_tz":-330},"id":"aG7HXFp6m9Io","outputId":"4bea73af-ba3b-40f8-c7ff-939330ff8e29"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"]}],"source":["import sys\n","print(sys.version)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1731146367845,"user":{"displayName":"Majdi Sukkar","userId":"11732602087870485821"},"user_tz":-330},"id":"j0Wkq126oP2e","outputId":"49d3cb81-397f-4d73-c421-69e9a1424663"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: COLAB_PYTHON_VERSION=3.9\n"]}],"source":["%env COLAB_PYTHON_VERSION=3.9\n","#!pip install python==3.9.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13696,"status":"ok","timestamp":1731147738345,"user":{"displayName":"Majdi Sukkar","userId":"11732602087870485821"},"user_tz":-330},"id":"sfJxf1hOie1t","outputId":"4b989b4a-69aa-4828-819d-a1f0fdd23336"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":23908,"status":"ok","timestamp":1731147781761,"user":{"displayName":"Majdi Sukkar","userId":"11732602087870485821"},"user_tz":-330},"id":"5mZCIeKMLmZD","outputId":"03ecd2df-c208-48cd-b367-c4480d15196a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting aequitas\n","  Downloading aequitas-1.0.0-3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: matplotlib>=3.7.5 in /usr/local/lib/python3.10/dist-packages (from aequitas) (3.8.0)\n","Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from aequitas) (2.2.2)\n","Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from aequitas) (6.0.2)\n","Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from aequitas) (0.13.2)\n","Collecting altair>=5.4.0 (from aequitas)\n","  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Collecting millify>=0.1.1 (from aequitas)\n","  Downloading millify-0.1.1.tar.gz (1.2 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from aequitas) (1.13.1)\n","Collecting optuna>=3.6.1 (from aequitas)\n","  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n","Collecting aif360>=0.6.1 (from aequitas)\n","  Downloading aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: lightgbm>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from aequitas) (4.5.0)\n","Collecting fairgbm>=0.9.14 (from aequitas)\n","  Downloading fairgbm-0.9.14-py3-none-any.whl.metadata (12 kB)\n","Collecting fairlearn>=0.10.0 (from aequitas)\n","  Downloading fairlearn-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n","Collecting hydra-core>=1.3.2 (from aequitas)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting hyperparameter-tuning>=0.3.1 (from aequitas)\n","  Downloading hyperparameter_tuning-0.3.2-py3-none-any.whl.metadata (4.2 kB)\n","Collecting validators>=0.33.0 (from aequitas)\n","  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from aequitas) (1.26.4)\n","Collecting fastparquet>=2024.2.0 (from aequitas)\n","  Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from aequitas) (2.32.3)\n","Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from aequitas) (2.5.0+cu121)\n","Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360>=0.6.1->aequitas) (1.5.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair>=5.4.0->aequitas) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=5.4.0->aequitas) (4.23.0)\n","Collecting narwhals>=1.5.2 (from altair>=5.4.0->aequitas)\n","  Downloading narwhals-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair>=5.4.0->aequitas) (24.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair>=5.4.0->aequitas) (4.12.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from fairgbm>=0.9.14->aequitas) (0.44.0)\n","Collecting cramjam>=2.3 (from fastparquet>=2024.2.0->aequitas)\n","  Downloading cramjam-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet>=2024.2.0->aequitas) (2024.10.0)\n","Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->aequitas)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->aequitas)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting schema (from hyperparameter-tuning>=0.3.1->aequitas)\n","  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (2.8.2)\n","Collecting alembic>=1.5.0 (from optuna>=3.6.1->aequitas)\n","  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna>=3.6.1->aequitas)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.6.1->aequitas) (2.0.36)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna>=3.6.1->aequitas) (4.66.6)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->aequitas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->aequitas) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->aequitas) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->aequitas) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->aequitas) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->aequitas) (2024.8.30)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->aequitas) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->aequitas) (3.4.2)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->aequitas) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->aequitas) (1.3.0)\n","Collecting Mako (from alembic>=1.5.0->optuna>=3.6.1->aequitas)\n","  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5.4.0->aequitas) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5.4.0->aequitas) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5.4.0->aequitas) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5.4.0->aequitas) (0.20.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.5->aequitas) (1.16.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360>=0.6.1->aequitas) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360>=0.6.1->aequitas) (3.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.6.1->aequitas) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair>=5.4.0->aequitas) (3.0.2)\n","Downloading aequitas-1.0.0-3-py3-none-any.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aif360-0.6.1-py3-none-any.whl (259 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading altair-5.4.1-py3-none-any.whl (658 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fairgbm-0.9.14-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fairlearn-0.11.0-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.3/232.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hyperparameter_tuning-0.3.2-py3-none-any.whl (26 kB)\n","Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading validators-0.34.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cramjam-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading narwhals-1.13.3-py3-none-any.whl (201 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n","Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, millify\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=8fd8074f4ff80f15bc9b48bab4cfad22369211ae2a4adeb1009663fcf9984c9d\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","  Building wheel for millify (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for millify: filename=millify-0.1.1-py3-none-any.whl size=1845 sha256=36811fb649b7b7a3817d3f9f5589cffdd65f008bbd2637c08887c4405b12f357\n","  Stored in directory: /root/.cache/pip/wheels/67/8f/53/2759feac2e247ce89c1165c3ff12d484de7714a875ea3464f0\n","Successfully built antlr4-python3-runtime millify\n","Installing collected packages: schema, millify, antlr4-python3-runtime, validators, omegaconf, narwhals, Mako, cramjam, colorlog, hydra-core, alembic, optuna, fastparquet, fairlearn, fairgbm, aif360, hyperparameter-tuning, altair, aequitas\n","  Attempting uninstall: altair\n","    Found existing installation: altair 4.2.2\n","    Uninstalling altair-4.2.2:\n","      Successfully uninstalled altair-4.2.2\n","Successfully installed Mako-1.3.6 aequitas-1.0.0 aif360-0.6.1 alembic-1.14.0 altair-5.4.1 antlr4-python3-runtime-4.9.3 colorlog-6.9.0 cramjam-2.9.0 fairgbm-0.9.14 fairlearn-0.11.0 fastparquet-2024.5.0 hydra-core-1.3.2 hyperparameter-tuning-0.3.2 millify-0.1.1 narwhals-1.13.3 omegaconf-2.3.0 optuna-4.0.0 schema-0.7.7 validators-0.34.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]},"id":"2d0b3ff9d63c484781b073ab542087e2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: from: command not found\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]}],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import tensorflow_probability as tfp\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","!pip install aequitas\n","import aequitas as ae\n","\n","# helper functions\n","!from utils import build_vocab_files, show_group_stats_viz, aggregate_dataset, preprocess_df, df_to_dataset, posterior_mean_field, prior_trainable\n","pd.set_option('display.max_columns', 500)"]},{"cell_type":"markdown","metadata":{"id":"OQ5o8In4LmZH"},"source":["## 1-2. Load data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2109,"status":"ok","timestamp":1731147783848,"user":{"displayName":"Majdi Sukkar","userId":"11732602087870485821"},"user_tz":-330},"id":"oMcjBhTZLmZJ","outputId":"38e213b1-a56a-419d-c215-8b59e63c6aad"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Student_project/Patient-Selection-for-Diabetes-Drug-Testing-main/data/final_project_dataset.csv\n","First few rows of the DataFrame:\n","+--------------+-------------+-----------------+--------+---------+--------+-------------------+--------------------------+---------------------+------------------+------------+--------------------------+------------------------+-----------------------+-------------------+------------------+------------------+--------------------+------------------+-----------------+----------------+------------+---------------+-----------+--------+------------+\n","| encounter_id | patient_nbr |      race       | gender |   age   | weight | admission_type_id | discharge_disposition_id | admission_source_id | time_in_hospital | payer_code |    medical_specialty     | primary_diagnosis_code | other_diagnosis_codes | number_outpatient | number_inpatient | number_emergency | num_lab_procedures | number_diagnoses | num_medications | num_procedures |  ndc_code  | max_glu_serum | A1Cresult | change | readmitted |\n","+--------------+-------------+-----------------+--------+---------+--------+-------------------+--------------------------+---------------------+------------------+------------+--------------------------+------------------------+-----------------------+-------------------+------------------+------------------+--------------------+------------------+-----------------+----------------+------------+---------------+-----------+--------+------------+\n","|   2278392    |   8222157   |    Caucasian    | Female | [0-10)  |   ?    |         6         |            25            |          1          |        1         |     ?      | Pediatrics-Endocrinology |         250.83         |          ?|?          |         0         |        0         |        0         |         41         |        1         |        1        |       0        |    nan     |      nan      |    nan    |   No   |     NO     |\n","|    149190    |  55629189   |    Caucasian    | Female | [10-20) |   ?    |         1         |            1             |          7          |        3         |     ?      |            ?             |          276           |      250.01|255       |         0         |        0         |        0         |         59         |        9         |       18        |       0        | 68071-1701 |      nan      |    nan    |   Ch   |    >30     |\n","|    64410     |  86047875   | AfricanAmerican | Female | [20-30) |   ?    |         1         |            1             |          7          |        2         |     ?      |            ?             |          648           |        250|V27        |         2         |        1         |        0         |         11         |        6         |       13        |       5        | 0378-1110  |      nan      |    nan    |   No   |     NO     |\n","|    500364    |  82442376   |    Caucasian    |  Male  | [30-40) |   ?    |         1         |            1             |          7          |        2         |     ?      |            ?             |           8            |      250.43|403       |         0         |        0         |        0         |         44         |        7         |       16        |       1        | 68071-1701 |      nan      |    nan    |   Ch   |     NO     |\n","|    16680     |  42519267   |    Caucasian    |  Male  | [40-50) |   ?    |         1         |            1             |          7          |        1         |     ?      |            ?             |          197           |        157|250        |         0         |        0         |        0         |         51         |        5         |        8        |       0        | 0049-4110  |      nan      |    nan    |   Ch   |     NO     |\n","|    16680     |  42519267   |    Caucasian    |  Male  | [40-50) |   ?    |         1         |            1             |          7          |        1         |     ?      |            ?             |          197           |        157|250        |         0         |        0         |        0         |         51         |        5         |        8        |       0        | 68071-1701 |      nan      |    nan    |   Ch   |     NO     |\n","|    35754     |  82637451   |    Caucasian    |  Male  | [50-60) |   ?    |         2         |            1             |          2          |        3         |     ?      |            ?             |          414           |        411|250        |         0         |        0         |        0         |         31         |        9         |       16        |       6        | 47918-902  |      nan      |    nan    |   No   |    >30     |\n","|    55842     |  84259809   |    Caucasian    |  Male  | [60-70) |   ?    |         3         |            1             |          2          |        4         |     ?      |            ?             |          414           |        411|V45        |         0         |        0         |        0         |         70         |        7         |       21        |       1        | 35208-001  |      nan      |    nan    |   Ch   |     NO     |\n","|    55842     |  84259809   |    Caucasian    |  Male  | [60-70) |   ?    |         3         |            1             |          2          |        4         |     ?      |            ?             |          414           |        411|V45        |         0         |        0         |        0         |         70         |        7         |       21        |       1        | 16729-001  |      nan      |    nan    |   Ch   |     NO     |\n","|    55842     |  84259809   |    Caucasian    |  Male  | [60-70) |   ?    |         3         |            1             |          2          |        4         |     ?      |            ?             |          414           |        411|V45        |         0         |        0         |        0         |         70         |        7         |       21        |       1        | 47918-891  |      nan      |    nan    |   Ch   |     NO     |\n","+--------------+-------------+-----------------+--------+---------+--------+-------------------+--------------------------+---------------------+------------------+------------+--------------------------+------------------------+-----------------------+-------------------+------------------+------------------+--------------------+------------------+-----------------+----------------+------------+---------------+-----------+--------+------------+\n","Number of rows: 143424\n","Number of columns: 26\n","Duration in seconds: 1.9324331283569336\n"]}],"source":["import pandas as pd\n","import time\n","from tabulate import tabulate\n","\n","dataset_path = \"/content/drive/MyDrive/Student_project/Patient-Selection-for-Diabetes-Drug-Testing-main/data/final_project_dataset.csv\"\n","print(dataset_path)\n","\n","# Read the CSV file and calculate start time\n","start_time = time.time()\n","df = pd.read_csv(dataset_path)\n","\n","# Print the first few rows of the DataFrame\n","print(\"First few rows of the DataFrame:\")\n","print(tabulate(df.head(10), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# Calculate and print the number of rows and columns\n","num_rows, num_columns = df.shape\n","print(\"Number of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n","\n","# create a copy dataframe\n","df_copy = df.copy()"]},{"cell_type":"markdown","metadata":{"id":"Kx3Zy1eZLmZR"},"source":["## 1-3. Level of dataset\n","\n","We need to confirm the level (line level, encounter level, or patient level) of our EHR datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"CMilNW87LmZT"},"outputs":[],"source":["# number of distinct encounter ids\n","num_unique_encounters = df['encounter_id'].nunique()\n","print(\"Number of unique encounters: {}\".format(num_unique_encounters))\n","\n","# number of rows in the dataset\n","print(\"Total number of rows: {}\".format(df.shape[0]))\n","\n","if num_unique_encounters == df.shape[0]:\n","    print(\"Data is at the Encounter Level\")\n","else:\n","    print(\"Data is at the Line Level\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1obRp5qJLmZX"},"outputs":[],"source":["import time\n","from tabulate import tabulate\n","\n","# Start time calculation\n","start_time = time.time()\n","\n","# this code snippet searches for the first patient\n","# who has more than one encounter and prints the data of the first encounters for that patient.\n","for patient_id in df['patient_nbr'].unique():\n","    patient_df = df[df['patient_nbr']==patient_id]\n","    if patient_df['encounter_id'].nunique() > 1:\n","        break\n","\n","# Print the first few rows of encounters for the selected patient\n","print(\"First few rows of encounters for the selected patient:\")\n","print(tabulate(patient_df.head(), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the number of rows and columns\n","num_rows, num_columns = df.shape\n","print(\"Number of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# Print the duration in seconds and minutes\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"markdown","metadata":{"id":"PVSJDtciLmZZ"},"source":["The number of the unique encounter_ids is smaller than the total number of rows in the dataset. Therefore, the dataset is at the **line level**.\n","\n","Besides `encounter_id` and `patient_nbr`, we should also group **all columns except the ndc_code columns** because those values are the same during the same encounter visit."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"V6s5uZDTLmZb"},"outputs":[],"source":["import time\n","from tabulate import tabulate\n","\n","# Assuming df is defined in your code\n","enc_cols = ['ndc_code']\n","enc_grps = [x for x in df.columns if x not in enc_cols]\n","\n","# Start time calculation\n","start_time = time.time()\n","\n","# Group into encounter level\n","enc_df = df.groupby(enc_grps)[enc_cols].agg(lambda x: list([y for y in x if y is not np.nan])).reset_index()\n","\n","# Print the first few rows of the DataFrame\n","print(\"First few rows of the DataFrame:\")\n","print(tabulate(enc_df.head(), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# Print the number of rows and columns\n","num_rows, num_columns = enc_df.shape\n","print(\"Number of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"markdown","metadata":{"id":"ucH4m93oLmZg"},"source":["We can confirm that this is not at the encounter level because the total number of rows is equal to the number of unique encounter_ids."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"MQdB-UiRLmZj"},"outputs":[],"source":["#These conditions are met if the number of rows in the DataFrame is equal to the number of unique values\n","# in the column \"encounter_id\". This implies that each value in the \"encounter_id\" column is unique,\n","# representing a distinct encounter.\n","enc_df.shape[0] == enc_df['encounter_id'].nunique()"]},{"cell_type":"markdown","metadata":{"id":"uOjdhL-sLmZm"},"source":["# 2. Exploratory Data Analysis\n","\n","## 2-1. Missing Values\n","\n","To explore the distributions of the features, I created a copy of the dataset and replaced \"?\" with np.nan"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3q96TCnrLmZn"},"outputs":[],"source":["from tabulate import tabulate\n","import time\n","\n","# Start time calculation\n","start_time = time.time()\n","\n","# Assuming enc_df is defined in your code\n","df_eda = enc_df.copy()\n","df_eda = df_eda.replace([\"?\", \"None\"], np.nan)\n","\n","# Print the first row of the DataFrame\n","print(\"First row of the DataFrame:\")\n","print(tabulate(df_eda.head(10), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# Print the number of rows and columns\n","num_rows, num_columns = df_eda.shape\n","print(\"\\nNumber of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"markdown","metadata":{"id":"gxhIjMhGLmZp"},"source":["From the cell below we see that:\n","\n","1. `weight` (96.8%), `payer_code` (39.5%), `medical_specialty` (49%), `max_glu_serum` (94.7%), and `A1Cresult` (83.2%) have high amount of missing vallues.\n","\n","2. `race` (2%) and `primary_diagnosis_code` (0.02%) have less missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ewGnhoiuLmZq"},"outputs":[],"source":["na_df = pd.DataFrame(df_eda.isna().sum(), columns=['num_nas'])\n","na_df['Percentage'] = na_df['num_nas']/df_eda.shape[0]\n","# Set display options for printing\n","pd.set_option('display.max_colwidth', None)\n","pd.set_option('display.max_columns', None)\n","\n","# Print the DataFrame with left alignment\n","na_df.style.set_properties(**{'text-align': 'left'})"]},{"cell_type":"markdown","metadata":{"id":"7uJMmS8_LmZs"},"source":["## 2-2. Distribution of Numeric Columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urdWMS4jLmZs"},"outputs":[],"source":["import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"EgQzbhwRLmZt"},"source":["The distribution of `age` is not Gaussian. The `weight` distribution looks like Gaussian. However, the distribution is truncated below 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVFm_nqCRpHL"},"outputs":[],"source":["# Replace missing values with medians\n","median_weight = df_eda['weight'].median()\n","df_eda['weight'].fillna(median_weight, inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3RLevaIOLmZu"},"outputs":[],"source":["# Distribution of Age and Weight\n","plt.figure(figsize=(15, 5))\n","\n","# Distribution of Age\n","plt.subplot(1, 1, 1)\n","ax = sns.countplot(x=\"age\", data=df_eda,\n","                   order=['[0-10)', '[10-20)', '[20-30)', '[30-40)', '[40-50)', '[50-60)',\n","                          '[60-70)', '[70-80)', '[80-90)', '[90-100)'])\n","ax.set_ylabel('Count of Patients')\n","plt.show()\n","\n","print(\" \" * 20)\n","# Distribution of Weight\n","plt.figure(figsize=(15, 5))\n","\n","plt.subplot(1, 1, 1)\n","ax = sns.countplot(x=\"weight\", data=df_eda,\n","                   order=['[0-25)', '[25-50)', '[50-75)', '[75-100)', '[100-125)', '[125-150)',\n","                          '[150-175)', '[175-200)', '>200'])\n","ax.set_ylabel('Count of Patients')\n","\n","# Add more visible horizontal lines between subplots\n","plt.axhline(y=0.5, color='black', linewidth=2, alpha=0.5)\n","plt.axhline(y=0.8, color='gray', linewidth=2, alpha=0.5)\n","plt.axhline(y=0.2, color='gray', linewidth=2, alpha=0.5)\n","\n","plt.show()\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(15, 5))\n","ax = sns.countplot(x=\"weight\", data=df_eda,\n","                   order=['[0-25)', '[25-50)', '[50-75)', '[75-100)', '[100-125)', '[125-150)',\n","                          '[150-175)', '[175-200)', '>200'])\n","ax.set_ylabel('Count of Patients')\n","\n","# Add more visible horizontal lines between subplots\n","for y in [0.2, 0.5, 0.8]:\n","    plt.axhline(y=y, color='gray', linewidth=2, alpha=0.5)\n","\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lYuMIhE_LmZv"},"source":["From the histograms below, we see that `num_lab_procedures` looks like Gaussian (however there's a second peak close to 0).\n","\n","On the other hand, `number_outpatient`, `number_inpatient`, `number_emergency`, `number_diagnoses`, `num_medications`, and `num_procedures` are not Gaussian distrubuted."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"vRzsHC9aLmZw"},"outputs":[],"source":["plt.figure(figsize=(5, 40))\n","\n","# distribution of number_inpatient\n","plt.subplot(8,1,1)\n","ax = sns.countplot(x=\"number_inpatient\", data=df_eda)\n","ax.set_ylabel('Count of Patients')\n","ax.tick_params(axis='both', which='major', labelsize=8)  # Reduce font size\n","ax.tick_params(axis='x', which='major', labelsize=5)  # Reduce font size on x-axis\n","# Draw a horizontal line at the end\n","plt.axhline(y=1, color='black', linewidth=0.5)  # Adjust line width\n","\n","# distribution of number_outpatient\n","plt.subplot(8,1,2)\n","ax = sns.countplot(x=\"number_outpatient\", data=df_eda)\n","ax.set_ylabel('Count of Patients')\n","ax.tick_params(axis='both', which='major', labelsize=8)  # Reduce font size\n","ax.tick_params(axis='x', which='major', labelsize=5)  # Reduce font size on x-axis\n","# Draw a horizontal line at the end\n","plt.axhline(y=1, color='black', linewidth=0.5)  # Adjust line width\n","\n","# distribution of number_emergency\n","plt.subplot(8,1,3)\n","ax = sns.countplot(x=\"number_emergency\", data=df_eda)\n","ax.set_ylabel('Count of Patients')\n","ax.tick_params(axis='both', which='major', labelsize=8)  # Reduce font size\n","ax.tick_params(axis='x', which='major', labelsize=5)  # Reduce font size on x-axis\n","# Draw a horizontal line at the end\n","plt.axhline(y=1, color='black', linewidth=0.5)  # Adjust line width\n","\n","# distribution of num_lab_procedures\n","plt.subplot(8,1,4)\n","ax = sns.countplot(x=\"num_lab_procedures\", data=df_eda)\n","ax.set_ylabel('Count of Patients')\n","ax.tick_params(axis='both', which='major', labelsize=8)  # Reduce font size\n","ax.tick_params(axis='x', which='major', labelsize=5)  # Reduce font size on x-axis\n","# Draw a horizontal line at the end\n","plt.axhline(y=1, color='black', linewidth=0.5)  # Adjust line width\n","\n","# distribution of number_diagnoses\n","plt.subplot(8,1,5)\n","ax = sns.countplot(x=\"number_diagnoses\", data=df_eda)\n","ax.set_ylabel('Count of Patients')\n","ax.tick_params(axis='both', which='major', labelsize=8)  # Reduce font size\n","ax.tick_params(axis='x', which='major', labelsize=5)  # Reduce font size on x-axis\n","# Draw a horizontal line at the end\n","plt.axhline(y=1, color='black', linewidth=0.5)  # Adjust line width\n","\n","# distribution of num_procedures\n","plt.subplot(8,1,6)\n","ax = sns.countplot(x=\"num_procedures\", data=df_eda)\n","ax.set_ylabel('Count of Patients')\n","ax.tick_params(axis='both', which='major', labelsize=8)  # Reduce font size\n","ax.tick_params(axis='x', which='major', labelsize=5)  # Reduce font size on x-axis\n","# Draw a horizontal line at the end\n","plt.axhline(y=1, color='black', linewidth=0.5)  # Adjust line width\n","\n","# distribution of num_medications\n","plt.subplot(8,1,7)\n","ax = sns.countplot(x=\"num_medications\", data=df_eda)\n","ax.set_ylabel('Count of Patients')\n","ax.tick_params(axis='both', which='major', labelsize=8)  # Reduce font size\n","ax.tick_params(axis='x', which='major', labelsize=5)  # Reduce font size on x-axis\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"CKg7PSpWLmZx"},"source":["For `max_glu_serum`, `A1Cresult`, and `readmitted`, there are only 3 distinct categories for each."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0NzsoI1wLmZy"},"outputs":[],"source":["plt.figure(figsize=(5,15))\n","\n","# distribution of max_glu_serum\n","plt.subplot(3,1,1)\n","ax = sns.countplot(x=\"max_glu_serum\", data=df_eda, color=\"blue\" )\n","ax.set_ylabel('Count of Patients')\n","\n","\n","# distribution of A1Cresult\n","plt.subplot(3,1,2)\n","ax = sns.countplot(x=\"A1Cresult\", data=df_eda, color=\"blue\")\n","ax.set_ylabel('Count of Patients')\n","\n","# distribution of readmitted\n","plt.subplot(3,1,3)\n","ax = sns.countplot(x=\"readmitted\", data=df_eda, color=\"blue\")\n","ax.set_ylabel('Count of Patients')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nYzqerqlLmZz"},"source":["Finally, the distribution of `time_in_hospital` (the value we want to predict) is not a Gaussian."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8WHFwPjjLmZ0"},"outputs":[],"source":["# distribution of time_in_hospital\n","plt.figure(figsize=(15,5))\n","ax = sns.countplot(x=\"time_in_hospital\", data=df_eda)\n","ax.set_ylabel('Count of Patients')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iumRYRhALmZ1"},"source":["# 3. Data Processing\n","\n","After understanding the data, we need to perform some processing for the following feature extraction. We will:\n","\n","1. Reduce the dimensionality of the NDC code (dims=251) to (dims=24)\n","\n","2. Reduce the number of rows by only selecting the first encounter of each patient\n","\n","3. Aggregate the dataframe by patient ID\n","\n","\n","## 3-1. Reduce Dimensionality of the NDC Code\n","\n","The dimensionality of the NDC codes can be reduced because the same drug can be represented by multiple codes."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZzYUlY7QLmZ2"},"outputs":[],"source":["import pandas as pd\n","from tabulate import tabulate\n","import time\n","\n","# Start time calculation\n","start_time = time.time()\n","\n","# NDC code lookup file\n","ndc_code_path = \"/content/drive/MyDrive/Student_project/Patient-Selection-for-Diabetes-Drug-Testing-main/medication_lookup_tables/final_ndc_lookup_table\"\n","ndc_code_df = pd.read_csv(ndc_code_path)\n","\n","# Print the first few rows of the DataFrame with left alignment\n","print(\"First few rows of the DataFrame:\")\n","print(tabulate(ndc_code_df.head(), headers='keys', tablefmt='pretty'))\n","\n","# Print the number of rows and columns\n","num_rows, num_columns = ndc_code_df.shape\n","print(\"\\nNumber of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"IfQyoXLCLmZ2"},"outputs":[],"source":["print(\"Number of unique ndc_code: {}\".format(df['ndc_code'].nunique()))\n","print(\"Number of unique non-proprietary names: {}\".format(ndc_code_df['Non-proprietary Name'].nunique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"VMyEA1-cLmZ3"},"outputs":[],"source":["# check if every ndc_code (except np.nan) exists in final_ndc_lookup_table\n","for x in df['ndc_code'].astype('str').unique():\n","    if x not in ndc_code_df['NDC_Code'].unique():\n","        print(\"{} is not in final_ndc_lookup_table\".format(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"S4GvzvwJLmZ4"},"outputs":[],"source":["%cd /content/drive/MyDrive/Student_project/Patient-Selection-for-Diabetes-Drug-Testing-main\n","\n","from student_utils import reduce_dimension_ndc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2FFtZxaLmZ6"},"outputs":[],"source":["reduce_dim_df = reduce_dimension_ndc(df, ndc_code_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsldFo1lLmZ8"},"outputs":[],"source":["# Number of unique values should be less for the new output field\n","assert df['ndc_code'].nunique() > reduce_dim_df['generic_drug_name'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3PC9LexkLmZ9"},"outputs":[],"source":["\n","# Start time calculation\n","start_time = time.time()\n","\n","# Assuming reduce_dim_df is defined in your code\n","# Print the first few rows of the DataFrame\n","print(\"First few rows of the DataFrame:\")\n","print(tabulate(reduce_dim_df.head(), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# Calculate and print the number of rows and columns\n","num_rows, num_columns = reduce_dim_df.shape\n","print(\"Number of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"eShRw8n9LmZ-"},"outputs":[],"source":["reduce_dim_df.shape"]},{"cell_type":"markdown","metadata":{"id":"cLwDoHbtLmaB"},"source":["## 3-2. Select the First Encounter for Each Patient\n","\n","For this project, we only use the data from the first encounter for each patient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ik6Xv3FYLmaB"},"outputs":[],"source":["from student_utils import select_first_encounter\n","\n","first_encounter_df = select_first_encounter(reduce_dim_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qVjlZd9TLmaU"},"outputs":[],"source":["# unique patients in transformed dataset\n","unique_patients = first_encounter_df['patient_nbr'].nunique()\n","print(\"Number of unique patients:{}\".format(unique_patients))\n","\n","# unique encounters in transformed dataset\n","unique_encounters = first_encounter_df['encounter_id'].nunique()\n","print(\"Number of unique encounters:{}\".format(unique_encounters))\n","\n","original_unique_patient_number = reduce_dim_df['patient_nbr'].nunique()\n","# number of unique patients should be equal to the number of unique encounters and patients in the final dataset\n","assert original_unique_patient_number == unique_patients\n","assert original_unique_patient_number == unique_encounters\n","print(\"Tests passed!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"WtatAtzgLmaW"},"outputs":[],"source":["print(first_encounter_df.shape)"]},{"cell_type":"markdown","metadata":{"id":"sW9WUsNMLmaX"},"source":["## 3-3. Aggregate Dataframe\n","\n","This code performs DataFrame aggregation. It first drops the column \"ndc_code\" from the DataFrame \"first_encounter_df\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gy4sSOR0LmaY"},"outputs":[],"source":["# drop ndc_code\n","first_encounter_df = first_encounter_df.drop('ndc_code', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04U8cl3Fe-Uv"},"outputs":[],"source":["print(first_encounter_df.shape)\n","first_encounter_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_b2DOHqgiNS"},"outputs":[],"source":["import utils"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"HsOMZjEsLmaZ"},"outputs":[],"source":["# التأكد من أن `grouping_field_list` هو قائمة\n","grouping_field_list = [c for c in first_encounter_df.columns]\n","grouping_field_list = list(grouping_field_list)\n","\n","# طباعة عدد العناصر في `grouping_field_list` و `first_encounter_df.columns`\n","print(\"عدد العناصر في grouping_field_list:\", len(grouping_field_list))\n","print(\"عدد العناصر في first_encounter_df.columns:\", len(first_encounter_df.columns))\n","\n","# التحقق من تطابق العمود في grouping_field_list مع الأعمدة في first_encounter_df\n","assert all(col in first_encounter_df.columns for col in grouping_field_list), \"Some columns in grouping_field_list are not in first_encounter_df\"\n","\n","# استدعاء الدالة مع التأكد من تحويل `grouping_field_list` إلى قائمة\n","#agg_drug_df, ndc_col_list = utils.aggregate_dataset(first_encounter_df, grouping_field_list, 'generic_drug_name')\n","#agg_drug_df = first_encounter_df.pivot_table(index=grouping_field_list, columns='generic_drug_name', aggfunc='size', fill_value=0)\n","#ndc_col_list = first_encounter_df.pivot_table(index=grouping_field_list, columns='generic_drug_name', aggfunc='size', fill_value=0)\n","#agg_drug_df, ndc_col_list = first_encounter_df.pivot_table(index=grouping_field_list, columns='generic_drug_name', aggfunc='size', fill_value=0), first_encounter_df.pivot_table(index=grouping_field_list, columns='generic_drug_name', aggfunc='size', fill_value=0).columns.tolist()\n","#agg_drug_df = first_encounter_df.groupby(grouping_field_list + ['generic_drug_name']).size().unstack(fill_value=0)\n","#ndc_col_list = agg_drug_df.columns.tolist()\n","\n","agg_drug_df = first_encounter_df.pivot_table(index=grouping_field_list, columns='generic_drug_name', aggfunc='size', fill_value=0)\n","ndc_col_list = agg_drug_df.columns.tolist()\n","\n","print(agg_drug_df.columns)\n","print(ndc_col_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8h1K9jbLmaZ"},"outputs":[],"source":["assert len(agg_drug_df) == agg_drug_df.index.nunique() == agg_drug_df.index.nunique()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Kdn_y7fILmaa"},"outputs":[],"source":["# show the list of generic drugs\n","ndc_col_list"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0gN728gMLmab"},"outputs":[],"source":["# inpect the data after aggregation\n","import time\n","import pandas as pd\n","from tabulate import tabulate\n","\n","# Start time calculation\n","start_time = time.time()\n","\n","# Assuming agg_drug_df is defined in your code\n","# Print the first few rows of the DataFrame\n","print(\"First few rows of the DataFrame:\")\n","print(tabulate(agg_drug_df.head(), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# Calculate and print the number of rows and columns\n","num_rows, num_columns = agg_drug_df.shape\n","print(\"Number of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"markdown","metadata":{"id":"jSOIpCXYLmac"},"source":["The dataset is now ready for feature selection."]},{"cell_type":"markdown","metadata":{"id":"bQIqhjzLLmac"},"source":["# 4. Feature Selection and Engineering\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZnCZFw34kli"},"outputs":[],"source":["agg_drug_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZTXBh0AZBPS"},"outputs":[],"source":["# التحقق من توفر عمود الوزن\n","if 'weight' in agg_drug_df.columns:\n","    # analysis of the weight data\n","    plt.figure(figsize=(15,5))\n","    weights_data = agg_drug_df['weight'].tolist()\n","    print(\"\\nDistribution of the available weight data:\")\n","    ax = sns.countplot(x=\"weight\", data=agg_drug_df,\n","                       order=['[0-25)','[25-50)','[50-75)','[75-100)','[100-125)','[125-150)',\n","                              '[150-175)', '[175-200)', '>200'])\n","    ax.set_ylabel('Count of Patients')\n","    plt.show()\n","    print(\"ratio of missing values of weights = {}\".format(weights_data.count('?')/len(weights_data)))\n","else:\n","    print(\"العمود 'weight' غير متوفر في البيانات.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqzQpCPDZK83"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Analysis of the weight data\n","plt.figure(figsize=(15, 5))\n","\n","# Extracting weight data\n","weights_data = agg_drug_df['weight'].tolist()\n","\n","# Displaying distribution of available weight data\n","print(\"\\nDistribution of available weight data:\")\n","ax = sns.countplot(x=\"weight\", data=agg_drug_df,\n","                   order=['[0-25)', '[25-50)', '[50-75)', '[75-100)', '[100-125)', '[125-150)',\n","                          '[150-175)', '[175-200)', '>200'])\n","ax.set_ylabel('Count of Patients')\n","plt.show()\n","\n","# Calculating the ratio of missing values of weights\n","missing_values_ratio = weights_data.count('?') / len(weights_data)\n","print(\"Ratio of missing values of weights = {}\".format(missing_values_ratio))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7Rm8CWJBwq5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Check if 'weight' column exists\n","if 'weight' in agg_drug_df.columns:\n","    # Analysis of the weight data\n","    plt.figure(figsize=(15, 5))\n","\n","    # Extracting weight data\n","    weights_data = agg_drug_df['weight'].tolist()\n","\n","    # Displaying distribution of available weight data\n","    print(\"\\nDistribution of available weight data:\")\n","    ax = sns.countplot(x=\"weight\", data=agg_drug_df,\n","                       order=['[0-25)', '[25-50)', '[50-75)', '[75-100)', '[100-125)', '[125-150)',\n","                              '[150-175)', '[175-200)', '>200'])\n","    ax.set_ylabel('Count of Patients')\n","    plt.show()\n","\n","    # Calculating the ratio of missing values of weights\n","    missing_values_ratio = weights_data.count('?') / len(weights_data)\n","    print(\"Ratio of missing values of weights = {}\".format(missing_values_ratio))\n","\n","else:\n","    print(\"Column 'weight' not found in the DataFrame.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"567lWGDyLmae"},"outputs":[],"source":["# analysis of the weight data\n","plt.figure(figsize=(15,5))\n","# Check if the 'weight' column exists before accessing it\n","if 'weight' in agg_drug_df.columns:\n","    weights_data = agg_drug_df['weight'].tolist()\n","    print(\"\\nDistribution of the availble weight data:\")\n","    ax = sns.countplot(x=\"weight\", data=agg_drug_df,\n","                       order=['[0-25)','[25-50)','[50-75)','[75-100)','[100-125)','[125-150)',\n","                              '[150-175)', '[175-200)', '>200'])\n","    ax.set_ylabel('Count of Patients')\n","    plt.show()\n","    print(\"ratio of missing values of weights = {}\".format(weights_data.count('?')/len(weights_data)))\n","else:\n","    print(\"Column 'weight' not found in DataFrame.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Nqmb29TLmaf"},"outputs":[],"source":["# required columns (Udacity)\n","required_demo_col_list = ['race', 'gender', 'age']\n","\n","# selected categorical columns (excluding ndc drugs)\n","\n","student_categorical_col_list = ['admission_source_id', 'max_glu_serum', 'A1Cresult'] + required_demo_col_list + ndc_col_list\n","\n","# selected numerical columns\n","student_numerical_col_list = ['num_medications', 'num_lab_procedures']\n","\n","PREDICTOR_FIELD = 'time_in_hospital'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9v-ffiALmag"},"outputs":[],"source":["def select_model_features(df, categorical_col_list, numerical_col_list, PREDICTOR_FIELD, grouping_key='patient_nbr'):\n","\n","    selected_col_list = [grouping_key] + [PREDICTOR_FIELD] + categorical_col_list + numerical_col_list\n","\n","    return agg_drug_df[selected_col_list]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1mUsTzceLk7"},"outputs":[],"source":["def select_model_features(df, categorical_cols, numerical_cols, target_col):\n","    # Select the desired columns\n","    selected_cols = categorical_cols + numerical_cols + [target_col]\n","\n","    # Filter the DataFrame to keep only the selected columns\n","    selected_df = df[selected_cols]\n","\n","    return selected_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSFXAwxdE7rV"},"outputs":[],"source":["print(agg_drug_df.columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFsp8BSHeWJC"},"outputs":[],"source":["def select_model_features(df, categorical_cols, numerical_cols, target_col):\n","    # Get a list of available columns\n","    available_columns = df.columns\n","\n","    # Check if all specified columns exist in the DataFrame\n","    for col in categorical_cols + numerical_cols + [target_col]:\n","        if col not in available_columns:\n","            raise ValueError(f\"Column '{col}' not found in the DataFrame.\")\n","\n","    selected_cols = categorical_cols + numerical_cols + [target_col]\n","\n","    # Filter the DataFrame to keep only the selected columns\n","    selected_df = df[selected_cols]\n","\n","    return selected_df\n","\n","# Assuming 'time_in_hospital' is in the agg_drug_df DataFrame\n","print(agg_drug_df.columns) # Print available columns to verify\n","\n","selected_features_df = select_model_features(agg_drug_df, # Changed from df to agg_drug_df\n","                                             ['Acarbose', 'Glimepiride', 'Glipizide', 'Glyburide',\n","                                              'Human Insulin',\n","                                              'Insulin Human', 'Nateglinide',\n","                                              'Pioglitazone', 'Repaglinide'],\n","                                             [],\n","                                             'time_in_hospital')\n","\n","# Verify the column names in your DataFrame\n","print(selected_features_df.columns) # Print the selected columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzR-Qo9lLmah"},"outputs":[],"source":["selected_features_df = select_model_features(agg_drug_df,\n","                                             student_categorical_col_list,\n","                                             student_numerical_col_list,\n","                                             PREDICTOR_FIELD)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEHuwfIXHVS6"},"outputs":[],"source":["def select_model_features(df, categorical_cols, numerical_cols, target_col):\n","    # Get a list of available columns\n","    available_columns = df.columns\n","\n","    # Check if all specified columns exist in the DataFrame\n","    for col in categorical_cols + numerical_cols + [target_col]:\n","        if col not in available_columns:\n","            print (f\"Warning: Column '{col}' not found in the DataFrame.\") # Changed from raise ValueError to print warning\n","            # Handle the missing column (e.g., skip it, raise an exception, etc.)\n","            # For now, we'll just skip the missing column\n","    selected_cols = [col for col in categorical_cols + numerical_cols + [target_col] if col in available_columns]\n","\n","    # Filter the DataFrame to keep only the selected columns\n","    selected_df = df[selected_cols]\n","\n","    return selected_df\n","\n","selected_features_df = select_model_features(agg_drug_df,\n","                                             student_categorical_col_list,\n","                                             student_numerical_col_list,\n","                                             PREDICTOR_FIELD)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"G-xISLQRLmai"},"outputs":[],"source":["# see the dataframe with selected features\n","\n","# Start time calculation\n","start_time = time.time()\n","\n","# Assuming selected_features_df is defined in your code\n","# Print the first few rows of the DataFrame\n","print(\"First few rows of the DataFrame:\")\n","print(tabulate(selected_features_df.head(), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# Calculate and print the number of rows and columns\n","num_rows, num_columns = selected_features_df.shape\n","print(\"Number of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"markdown","metadata":{"id":"bUJb0cQ2Lmai"},"source":["## 4-2. Impute Numerical Features by Zeros"]},{"cell_type":"markdown","metadata":{"id":"7FXRquFWu74A"},"source":["The code aims to process the input DataFrame using this information and perform appropriate operations such as handling missing values and converting categorical columns into numerical ones for use in analysis or prediction tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DZ54fWCLmaj"},"outputs":[],"source":["processed_df = utils.preprocess_df(selected_features_df.copy(),\n","                             student_categorical_col_list,\n","                             student_numerical_col_list,\n","                             PREDICTOR_FIELD,\n","                             categorical_impute_value='nan',\n","                             numerical_impute_value=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmmpr1X2Hn8A"},"outputs":[],"source":["def select_model_features(df, categorical_cols, numerical_cols, target_col):\n","    # Get a list of available columns\n","    available_columns = df.columns\n","\n","    # Check if all specified columns exist in the DataFrame\n","    missing_cols = [] # Initialize a list to store missing columns\n","    for col in categorical_cols + numerical_cols + [target_col]:\n","        if col not in available_columns:\n","            missing_cols.append(col) # Add missing column to the list\n","            print (f\"Warning: Column '{col}' not found in the DataFrame.\") # Changed from raise ValueError to print warning\n","            # Handle the missing column (e.g., skip it, raise an exception, etc.)\n","\n","    selected_cols = [col for col in categorical_cols + numerical_cols + [target_col] if col in available_columns]\n","\n","    # Filter the DataFrame to keep only the selected columns\n","    selected_df = df[selected_cols]\n","\n","    return selected_df, missing_cols # Return the DataFrame and the list of missing columns\n","\n","selected_features_df, missing_cols = select_model_features(agg_drug_df,\n","                                             student_categorical_col_list,\n","                                             student_numerical_col_list,\n","                                             PREDICTOR_FIELD)\n","\n","# Check if 'time_in_hospital' is in the list of missing columns and handle accordingly\n","if 'time_in_hospital' in missing_cols:\n","    # Handle the missing column (e.g., add it to the DataFrame with a default value, etc.)\n","    # For example, add the column with a default value of 0\n","    selected_features_df['time_in_hospital'] = 0"]},{"cell_type":"markdown","metadata":{"id":"hsY7hv1mLmak"},"source":["## 4-3. Split Data\n","\n","We split the data into 60% training dataset, 20% validation dataset, and 20% test dataset. After split, we confirm that there's not data leakage between the partitions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gShxyMSFIO5I"},"outputs":[],"source":["import pandas as pd\n","\n","# Load or create your DataFrame\n","processed_df = pd.read_csv(dataset_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1mE4ZwgLmal"},"outputs":[],"source":["import student_utils\n","#from student_utils import patient_dataset_splitter\n","\n","d_train, d_val, d_test = student_utils.patient_dataset_splitter(processed_df, 'patient_nbr')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ve3Klug6Lmam"},"outputs":[],"source":["assert len(d_train) + len(d_val) + len(d_test) == len(processed_df)\n","print(\"Test passed for number of total rows equal!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"wQvBvDsTLman"},"outputs":[],"source":["assert (d_train['patient_nbr'].nunique() + d_val['patient_nbr'].nunique() + d_test['patient_nbr'].nunique()) == agg_drug_df['patient_nbr'].nunique()\n","print(\"Test passed for number of unique patients being equal!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SU3YGb54I3Le"},"outputs":[],"source":["# Check if 'patient_nbr' exists in the agg_drug_df columns\n","print(agg_drug_df.columns)\n","\n","# If not found, try to find the correct column name or recreate the column\n","# For example, if you have a similar column like 'patient_id', you could rename it:\n","agg_drug_df = agg_drug_df.rename(columns={'patient_id': 'patient_nbr'}) #Rename the column from 'patient_id' to 'patient_nbr'\n","\n","# Or, if you need to recreate it based on other columns, you can do so:\n","# For example, if you have 'encounter_id' and can extract patient number from it:\n","# agg_drug_df['patient_nbr'] = agg_drug_df['encounter_id'].str.extract(r'(\\d+)')\n","\n","# After fixing the column, rerun the assertion\n","assert (d_train['patient_nbr'].nunique() + d_val['patient_nbr'].nunique() + d_test['patient_nbr'].nunique()) == agg_drug_df['patient_nbr'].nunique()\n","print(\"Test passed for number of unique patients being equal!\")"]},{"cell_type":"markdown","metadata":{"id":"hpqFvEKbLman"},"source":["## 4-4. Demographic Representation Analysis of Split"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"I9GTqMYWLmao"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","utils.show_group_stats_viz(processed_df, PREDICTOR_FIELD)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"_aOPtSBaLmap"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","utils.show_group_stats_viz(d_train, PREDICTOR_FIELD)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"vgEk-zaLLmaq"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","utils.show_group_stats_viz(d_test, PREDICTOR_FIELD)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"N-vRTqLDLmas"},"outputs":[],"source":["# Full dataset before splitting\n","plt.figure(figsize=(5,5))\n","patient_demo_features = ['race', 'gender', 'age', 'patient_nbr']\n","patient_group_analysis_df = processed_df[patient_demo_features].groupby('patient_nbr').head(1).reset_index(drop=True)\n","utils.show_group_stats_viz(patient_group_analysis_df, 'gender')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"aUR0E7MQLmat"},"outputs":[],"source":["# Training partition\n","plt.figure(figsize=(5,5))\n","utils.show_group_stats_viz(d_train, 'gender')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"2P0VHzNBLmau"},"outputs":[],"source":["# Test partition\n","plt.figure(figsize=(5,5))\n","utils.show_group_stats_viz(d_test, 'gender')"]},{"cell_type":"markdown","metadata":{"id":"6szOReVbLmav"},"source":["## 4-5. Convert Dataset to TF Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RJXJfxsLmav"},"outputs":[],"source":["# convert dataset from Pandas dataframes to TF dataset\n","batch_size = 128\n","diabetes_train_ds = utils.df_to_dataset(d_train, PREDICTOR_FIELD, batch_size=batch_size)\n","diabetes_val_ds = utils.df_to_dataset(d_val, PREDICTOR_FIELD, batch_size=batch_size)\n","diabetes_test_ds = utils.df_to_dataset(d_test, PREDICTOR_FIELD, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APkqfpEXJWPZ"},"outputs":[],"source":["# utils.py\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","\n","def df_to_dataset(df, predictor, batch_size):\n","    df = df.copy()\n","    labels = df.pop(predictor)\n","    # Convert 'object' columns to numerical if possible\n","    for col in df.select_dtypes(include=['object']):\n","        try:\n","            df[col] = pd.to_numeric(df[col])\n","        except ValueError:\n","            pass  # Handle columns that cannot be converted\n","    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n","    ds = ds.shuffle(buffer_size=len(df))\n","    ds = ds.batch(batch_size)\n","    return ds"]},{"cell_type":"markdown","metadata":{"id":"bw_ol7RxXxbO"},"source":["These messages are notifications indicating that the vocabulary size for some features has been inferred based on the number of elements in the corresponding vocabulary file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAYVTPTZLmaw"},"outputs":[],"source":["# We use this sample of the dataset to show transformations later\n","diabetes_batch = next(iter(diabetes_train_ds))[0]\n","def demo(feature_column, example_batch):\n","    feature_layer = layers.DenseFeatures(feature_column)\n","    print(feature_layer(example_batch))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdT8ci-sJi4c"},"outputs":[],"source":["# Example values - replace these with your actual data\n","d_train = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6], 'target': [7, 8, 9]})\n","d_val = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6], 'target': [7, 8, 9]}) # Added this line\n","d_test = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6], 'target': [7, 8, 9]}) # Added this line\n","PREDICTOR_FIELD = 'target'\n","\n","# convert dataset from Pandas dataframes to TF dataset\n","batch_size = 128\n","diabetes_train_ds = utils.df_to_dataset(d_train, PREDICTOR_FIELD, batch_size=batch_size)\n","diabetes_val_ds = utils.df_to_dataset(d_val, PREDICTOR_FIELD, batch_size=batch_size)\n","diabetes_test_ds = utils.df_to_dataset(d_test, PREDICTOR_FIELD, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"M_bTKfrzLmax"},"source":["## 4-6. Create Categorical Features with TF Feature Columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NECGvq7JLmay"},"outputs":[],"source":["# we adding the directory\n","#!mkdir diabetes_vocab\n","vocab_file_list = utils.build_vocab_files(d_train, student_categorical_col_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7pk4uU4J4X2"},"outputs":[],"source":["# we adding the directory\n","#!mkdir diabetes_vocab\n","# Check the columns in d_train\n","print(d_train.columns)\n","\n","# Update student_categorical_col_list with valid column names from d_train\n","student_categorical_col_list = ['feature1', 'feature2']\n","\n","vocab_file_list = utils.build_vocab_files(d_train, student_categorical_col_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"jNOkI-SoLmaz"},"outputs":[],"source":["# view the list of vocab files\n","vocab_file_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QdGY9gPLma0"},"outputs":[],"source":["from student_utils import create_tf_categorical_feature_cols\n","\n","# Replace 'student_utils' with 'utils' assuming that's the correct import\n","tf_cat_col_list = create_tf_categorical_feature_cols(student_categorical_col_list)\n"]},{"cell_type":"markdown","metadata":{"id":"ufrphzfSLma1"},"source":["## 4-7. Create Numerical Features"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"6jfv1BObfL8B"},"outputs":[],"source":["for col in student_numerical_col_list:\n","    d_train[col] = d_train[col].astype('float32')\n","    d_val[col] = d_val[col].astype('float32')\n","    d_test[col] = d_test[col].astype('float32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvfVmPgOKFLa"},"outputs":[],"source":["for col in student_numerical_col_list:\n","    # Check if the column exists before accessing it\n","    if col in d_train.columns:\n","        d_train[col] = d_train[col].astype('float32')\n","    if col in d_val.columns:\n","        d_val[col] = d_val[col].astype('float32')\n","    if col in d_test.columns:\n","        d_test[col] = d_test[col].astype('float32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxVvRIbBLma3"},"outputs":[],"source":["import student_utils\n","import functools\n","from student_utils import normalize_numeric_with_zscore, create_tf_numeric_feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wh7E_6JhLma4"},"outputs":[],"source":["def calculate_stats_from_train_data(df, col):\n","    mean = df[col].describe()['mean']\n","    std = df[col].describe()['std']\n","    return mean, std\n","\n","def create_tf_numerical_feature_cols(numerical_col_list, train_df):\n","    tf_numeric_col_list = []\n","    for c in numerical_col_list:\n","        mean, std = calculate_stats_from_train_data(train_df, c)\n","        tf_numeric_feature = create_tf_numeric_feature(c, mean, std)\n","        tf_numeric_col_list.append(tf_numeric_feature)\n","    return tf_numeric_col_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_o0E00gLma4"},"outputs":[],"source":["tf_cont_col_list = create_tf_numerical_feature_cols(student_numerical_col_list, d_train)"]},{"cell_type":"markdown","metadata":{"id":"OR66JrFBLma6"},"source":["## 4-8. Combine Features with DenseFeatures"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCeycPWHLma6"},"outputs":[],"source":["# concatenate categorical and numeric feature columns\n","claim_feature_columns = tf_cat_col_list + tf_cont_col_list\n","\n","# combine with TF DenseFeatures\n","claim_feature_layer = tf.keras.layers.DenseFeatures(claim_feature_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASSBxVs8KcT8"},"outputs":[],"source":["def calculate_stats_from_train_data(df, col):\n","    # Check if column exists in DataFrame\n","    if col in df.columns:\n","        mean = df[col].describe()['mean']\n","        std = df[col].describe()['std']\n","        return mean, std\n","    else:\n","        # Handle missing column (e.g., return default values or raise an exception)\n","        return 0, 1  # Example: return 0 for mean and 1 for std\n","\n","def create_tf_numerical_feature_cols(numerical_col_list, train_df):\n","    tf_numeric_col_list = []\n","    for c in numerical_col_list:\n","        # Calculate stats only if the column exists\n","        mean, std = calculate_stats_from_train_data(train_df, c)\n","        tf_numeric_feature = create_tf_numeric_feature(c, mean, std)\n","        tf_numeric_col_list.append(tf_numeric_feature)\n","    return tf_numeric_col_list"]},{"cell_type":"markdown","metadata":{"id":"pyJV--LYLma7"},"source":["# 5. Build and Train the Model\n","\n","The model is built with DenseFeatures and TF Probability Layers. The optimizer used is rmsprop. We train the model for 10 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKh5SHDg_fVL"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","source":[],"metadata":{"id":"AaIMP9u2sbba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Dense, LayerNormalization, MultiHeadAttention, Dropout\n","\n","# Define Transformer Block\n","class TransformerBlock(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = tf.keras.Sequential(\n","            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n","        )\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# Create Transformer-based model for EHR data\n","def build_transformer_model(input_shape, num_heads=4, embed_dim=32, ff_dim=128):\n","    inputs = tf.keras.Input(shape=input_shape)\n","    transformer_block = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)\n","    x = transformer_block(inputs)\n","    x = Dense(64, activation=\"relu\")(x)\n","    x = Dropout(0.5)(x)\n","    outputs = Dense(1, activation=\"sigmoid\")(x)  # For binary classification (diabetes)\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"AUC\", \"F1\", \"Precision\", \"Recall\", \"Accuracy\"])\n","    return model\n"],"metadata":{"id":"OMFlXxGxpo6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install graphviz\n","import graphviz\n","\n","def visualize_transformer_model():\n","    # Create a new Digraph object for the structure\n","    dot = graphviz.Digraph(comment='Transformer-based Model')\n","\n","    # Add input layer\n","    dot.node('Input', 'Input Layer\\nShape: (input_shape)', shape='ellipse')\n","\n","    # Transformer Block\n","    dot.node('Transformer', 'Transformer Block\\n(Multi-Head Attention, FFN)', shape='box')\n","    dot.node('LayerNorm1', 'LayerNorm1', shape='box')\n","    dot.node('Dropout1', 'Dropout1', shape='box')\n","    dot.node('FFN', 'Feed Forward Network\\n(Dense layers)', shape='box')\n","    dot.node('LayerNorm2', 'LayerNorm2', shape='box')\n","    dot.node('Dropout2', 'Dropout2', shape='box')\n","\n","    # Dense and Dropout Layers after Transformer Block\n","    dot.node('Dense64', 'Dense Layer (64 units)\\nActivation: ReLU', shape='box')\n","    dot.node('Dropout3', 'Dropout (0.5)', shape='box')\n","\n","    # Output Layer\n","    dot.node('Output', 'Output Layer\\nActivation: Sigmoid\\n(Binary Classification)', shape='ellipse')\n","\n","    # Connect the layers\n","    dot.edge('Input', 'Transformer')\n","    dot.edge('Transformer', 'LayerNorm1')\n","    dot.edge('LayerNorm1', 'Dropout1')\n","    dot.edge('Dropout1', 'FFN')\n","    dot.edge('FFN', 'LayerNorm2')\n","    dot.edge('LayerNorm2', 'Dropout2')\n","    dot.edge('Dropout2', 'Dense64')\n","    dot.edge('Dense64', 'Dropout3')\n","    dot.edge('Dropout3', 'Output')\n","\n","    # Render the graph to a file or display it\n","    dot.render('transformer_model_structure', view=True, format='png')  # Save and open as a PNG file\n","\n","# Call the function to generate the structure diagram\n","visualize_transformer_model()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxEYlVnIFaSD","executionInfo":{"status":"ok","timestamp":1728105174792,"user_tz":-330,"elapsed":4709,"user":{"displayName":"Majdi Sukkar","userId":"11732602087870485821"}},"outputId":"5b9948f6-7bbb-4814-e2d6-50ed0145b1aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n"]}]},{"cell_type":"code","source":["# Make sure graphviz is installed using: !pip install graphviz\n","import graphviz\n","\n","def visualize_hybrid_cnn_transformer_model():\n","    # Create a new Digraph object for the structure\n","    dot = graphviz.Digraph(comment='Hybrid CNN-Transformer Model')\n","\n","    # Add input layer\n","    dot.node('Input', 'Input Layer\\nShape: (input_shape)', shape='ellipse')\n","\n","    # CNN Block\n","    dot.node('Conv1', 'Conv2D Layer\\n(32 filters, 3x3 kernel)\\nActivation: ReLU', shape='box')\n","    dot.node('MaxPool1', 'MaxPooling2D\\n(2x2 pool size)', shape='box')\n","    dot.node('Conv2', 'Conv2D Layer\\n(64 filters, 3x3 kernel)\\nActivation: ReLU', shape='box')\n","    dot.node('MaxPool2', 'MaxPooling2D\\n(2x2 pool size)', shape='box')\n","    dot.node('Flatten', 'Flatten Layer', shape='box')\n","\n","    # Transformer Block\n","    dot.node('Transformer', 'Transformer Block\\n(Multi-Head Attention, FFN)', shape='box')\n","    dot.node('LayerNorm1', 'LayerNorm1', shape='box')\n","    dot.node('Dropout1', 'Dropout1', shape='box')\n","    dot.node('FFN', 'Feed Forward Network\\n(Dense layers)', shape='box')\n","    dot.node('LayerNorm2', 'LayerNorm2', shape='box')\n","    dot.node('Dropout2', 'Dropout2', shape='box')\n","\n","    # Dense and Dropout Layers after Transformer Block\n","    dot.node('Dense64', 'Dense Layer (64 units)\\nActivation: ReLU', shape='box')\n","    dot.node('Dropout3', 'Dropout (0.5)', shape='box')\n","\n","    # Output Layer\n","    dot.node('Output', 'Output Layer\\nActivation: Sigmoid\\n(Binary Classification)', shape='ellipse')\n","\n","    # Connect the CNN layers\n","    dot.edge('Input', 'Conv1')\n","    dot.edge('Conv1', 'MaxPool1')\n","    dot.edge('MaxPool1', 'Conv2')\n","    dot.edge('Conv2', 'MaxPool2')\n","    dot.edge('MaxPool2', 'Flatten')\n","\n","    # Connect CNN block to Transformer Block\n","    dot.edge('Flatten', 'Transformer')\n","\n","    # Connect the layers within the Transformer block\n","    dot.edge('Transformer', 'LayerNorm1')\n","    dot.edge('LayerNorm1', 'Dropout1')\n","    dot.edge('Dropout1', 'FFN')\n","    dot.edge('FFN', 'LayerNorm2')\n","    dot.edge('LayerNorm2', 'Dropout2')\n","\n","    # Connect Transformer block to final dense layers\n","    dot.edge('Dropout2', 'Dense64')\n","    dot.edge('Dense64', 'Dropout3')\n","    dot.edge('Dropout3', 'Output')\n","\n","    # Render the graph to a file or display it\n","    dot.render('hybrid_cnn_transformer_model_structure', view=True, format='png')  # Save and open as a PNG file\n","\n","# Call the function to generate the structure diagram\n","visualize_hybrid_cnn_transformer_model()\n"],"metadata":{"id":"0EjqJLstTe3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_hybrid_cnn_transformer_model(input_shape):\n","    # Initialize model with CNN and Transformer layers\n","    model = initialize_model(input_shape)\n","\n","    # Loop over the number of epochs\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","\n","        # Loop over each training example (x, y)\n","        for x, y in training_data:\n","            # Forward propagation\n","            # Pass inputs through the CNN block to extract local features\n","            cnn_features = cnn_block(x)\n","            print(\"Extracted local features from CNN\")\n","\n","            # Pass local features through the Transformer block\n","            transformer_output = transformer_block(cnn_features)\n","            print(\"Passed features through Transformer block\")\n","\n","            # Apply Dense layers and activation functions\n","            outputs = dense_layers(transformer_output)\n","            print(\"Applied Dense layers and activation functions\")\n","\n","            # Compute the loss\n","            loss = compute_loss(outputs, y)\n","            print(f\"Loss computed: {loss}\")\n","\n","            # Backpropagation\n","            # Compute gradients of loss with respect to model parameters\n","            model = backpropagate(loss)\n","            print(\"Backpropagation completed, gradients updated\")\n","\n","    # Return the trained model\n","    return model\n"],"metadata":{"id":"1-8zqsr3V4sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"LExfoz9xsZtU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1ODnqHQKrPA"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_probability as tfp\n","from tensorflow.keras import layers\n","\n","# Define the function to create the DenseVariational layer\n","def get_dense_variational():\n","    # Define the prior distribution for the kernel and bias\n","    def prior(kernel_size, bias_size=0, dtype=None):\n","        n = kernel_size + bias_size\n","        return tf.keras.Sequential([\n","            tfp.layers.VariableLayer(2 * n, dtype=dtype),\n","            tfp.layers.DistributionLambda(\n","                lambda t: tfp.distributions.Independent(\n","                    tfp.distributions.Normal(\n","                        loc=t[..., :n], scale=1e-5 + tf.nn.softplus(0.01 * t[..., n:])\n","                    ),\n","                    reinterpreted_batch_ndims=1,\n","                )\n","            ),\n","        ])\n","\n","    # Define the posterior distribution for the kernel and bias\n","    def posterior(kernel_size, bias_size=0, dtype=None):\n","        n = kernel_size + bias_size\n","        return tf.keras.Sequential([\n","            tfp.layers.VariableLayer(2 * n, dtype=dtype),\n","            tfp.layers.DistributionLambda(\n","                lambda t: tfp.distributions.Independent(\n","                    tfp.distributions.Normal(\n","                        loc=t[..., :n], scale=1e-5 + tf.nn.softplus(0.01 * t[..., n:])\n","                    ),\n","                    reinterpreted_batch_ndims=1,\n","                )\n","            ),\n","        ])\n","\n","    # Create the DenseVariational layer\n","    return tfp.layers.DenseVariational(\n","        units=1, # Example output units\n","        make_prior_fn=None,\n","        make_posterior_fn=None,\n","        kl_weight=None,\n","        activation=None,\n","        activity_regularizer=None,\n","        kernel_constraint=None,\n","        bias_constraint=None,\n","        kernel_prior_fn=None,\n","        bias_prior_fn=None,\n","        kernel_posterior_fn=None,\n","        bias_posterior_fn=None,\n","        kl_divergence_function=None\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"movsX2OV6de_"},"outputs":[],"source":["#Parallel\n","# Record the start time of the training\n","import time\n","start_time = time.time()\n","\n","# Build and train the model\n","diabetes_model, history = build_diabetes_model(diabetes_train_ds, diabetes_val_ds, claim_feature_layer, epochs=20)\n","\n","# Record the end time of the training\n","end_time = time.time()\n","\n","# Calculate the training time in seconds\n","training_time_seconds = end_time - start_time\n","\n","# Calculate the training time in minutes and seconds\n","training_time_minutes = training_time_seconds / 60\n","\n","# Print the training time\n","print(\"Training time: {:.2f} seconds\".format(training_time_seconds))\n","print(\"Training time: {:.2f} minutes\".format(training_time_minutes))\n"]},{"cell_type":"markdown","metadata":{"id":"lj1yDIf81uA5"},"source":["fluctuations in validation MSE during training can be due to changes in dataset composition, data irregularities, model parameter tuning, random sampling effects, and environmental or noise effects. These factors interact to influence model performance across epochs."]},{"cell_type":"markdown","metadata":{"id":"wtsVQqniLma_"},"source":["# 6. Results\n","\n","## 6-1. Uncertainty of Model Prediction\n","\n","We use TF Probability Layer to build the model. As a result, we can extract the mean and standard deviation for each prediction the model made."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"imGBc9roLma_"},"outputs":[],"source":["# list of columns used as features\n","feature_list = student_categorical_col_list + student_numerical_col_list\n","#\n","diabetes_x_tst = dict(d_test[feature_list])\n","#\n","diabetes_yhat = diabetes_model(diabetes_x_tst)\n","\n","# predict on the test dataset\n","preds = diabetes_model.predict(diabetes_test_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3IKilNILmbA"},"outputs":[],"source":["from student_utils import get_mean_std_from_preds\n","m, s = get_mean_std_from_preds(diabetes_yhat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9F0xL8mxLmbA"},"outputs":[],"source":["# create a dict for building the dataframe\n","prob_outputs = {\"pred\": preds.flatten(),\n","                \"actual_value\": d_test['time_in_hospital'].values,\n","                \"pred_mean\": m.numpy().flatten(),\n","                \"pred_std\": s.numpy().flatten()}\n","\n","# convert prob_outputs into a dataframe\n","prob_output_df = pd.DataFrame(prob_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lskNvhi2LmbB"},"outputs":[],"source":["# check the results\n","# Start time calculation\n","start_time = time.time()\n","\n","# Assuming selected_features_df is defined in your code\n","# Print the first few rows of the DataFrame\n","print(\"First few rows of the DataFrame:\")\n","print(tabulate(selected_features_df.head(), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# Calculate and print the number of rows and columns\n","num_rows, num_columns = selected_features_df.shape\n","print(\"Number of rows:\", num_rows)\n","print(\"Number of columns:\", num_columns)\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"markdown","metadata":{"id":"9utYaq7KLmbC"},"source":["## 6-2. Patient Selection"]},{"cell_type":"markdown","metadata":{"id":"en7VISysLmbD"},"source":["We use the mean of the prediction to perform binary classification on each patients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gha_s1qSLmbD"},"outputs":[],"source":["from student_utils import get_student_binary_prediction\n","\n","student_binary_prediction = get_student_binary_prediction(prob_output_df, 'pred_mean')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sc84JlD8LmbE"},"outputs":[],"source":["def add_pred_to_test(test_df, pred_np, demo_col_list):\n","    \"\"\"\n","    Create a dataframe for the test dataset with 'score' and 'label_value'.\n","    'score' is the binary prediction that a patient would spend >= 5 days in the hospital\n","    'label_value' is the ground truth that a patient spent >= 5 days in the hospital\n","    Args:\n","        test_df - the test dataset\n","        pred_np - the binary prediction of the test dataset by the model\n","        demo_col_list: the categorical feature names\n","    Output:\n","        test_df - dataframe of the test dataset (with score and label_value)\n","    \"\"\"\n","\n","    # Create a copy of the dataframe to avoid SettingWithCopyWarning\n","    test_df = test_df.copy()\n","\n","    # convert all values of the categorical columns into strings\n","    for c in demo_col_list:\n","        test_df[c] = test_df[c].astype(str)\n","\n","    # model binary prediction\n","    test_df['score'] = pred_np\n","\n","    # actual labels\n","    test_df['label_value'] = test_df['time_in_hospital'].apply(lambda x: 1 if x >= 5 else 0)\n","\n","    return test_df\n","\n","# Use the modified function\n","pred_test_df = add_pred_to_test(d_test, student_binary_prediction, ['race', 'gender'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"RRClBlG3LmbF"},"outputs":[],"source":["# Start time calculation\n","start_time = time.time()\n","\n","# Assuming pred_test_df is defined in your code\n","# Print the selected columns of the first few rows of the DataFrame\n","print(\"Selected columns of the first few rows of the DataFrame:\")\n","print(tabulate(pred_test_df[['patient_nbr', 'gender', 'race', 'time_in_hospital', 'score', 'label_value']].head(), headers='keys', tablefmt='pretty', showindex=False))\n","\n","# End time calculation\n","end_time = time.time()\n","\n","# Calculate the duration\n","duration_seconds = end_time - start_time\n","\n","# Print the duration in seconds\n","print(\"Duration in seconds:\", duration_seconds)\n"]},{"cell_type":"markdown","metadata":{"id":"g-eKnMa_YcKw"},"source":[]},{"cell_type":"markdown","metadata":{"id":"AlcQljT8LmbH"},"source":["# 7. Model Evaluation\n","\n","## 7-1. Precision and Recall\n","\n","Precision is the ratio of the number of true positives to the sum of the true positives and false positives. Recall is the ratio of the number of true positives to the sum of the true positives and false negatives. There's a tradeoff between them. If we increase the precision (we say no to the data that we're less confident in classifying as positive), we will classfy more true positives as negative and as a result decreasing the recall. On the other hand, if we increase the recall (by saying yes to the data that we're less confident in classifying it as as negative), we will calssify more true negative data as positive and as a result decreasing the precision.\n","\n","To further improve the model's performance, we could focus on enhancing the recall without significantly sacrificing precision. One approach could be to adjust the classification threshold to favor sensitivity over specificity, thereby increasing the likelihood of capturing more positive cases. Additionally, we might explore feature engineering techniques to extract more relevant information from the data, which could potentially improve the model's ability to identify positive cases. Moreover, experimenting with different algorithms or model architectures might yield better results. Finally, incorporating techniques such as ensemble learning or model stacking could also help in boosting performance by combining the strengths of multiple models."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"G7WAOyjiLmbI"},"outputs":[],"source":["from tabulate import tabulate\n","import pandas as pd\n","from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n","\n","y_true = pred_test_df['label_value']\n","y_pred = pred_test_df['score']\n","\n","# AUC, F1, precision and recall\n","# Summary\n","\n","auc = roc_auc_score(y_true, y_pred)\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred)\n","recall = recall_score(y_true, y_pred)\n","\n","# Create a DataFrame to display the results\n","results_df = pd.DataFrame({\n","    'Metric': ['AUC', 'F1 score', 'Precision', 'Recall'],\n","    'Value': [auc, f1, precision, recall]\n","})\n","\n","# Convert DataFrame to a list of lists for tabulate\n","results_list = results_df.values.tolist()\n","\n","# Print the results using tabulate\n","print(tabulate(results_list, headers=['Metric ', 'Value'], tablefmt='pretty'))\n"]},{"cell_type":"markdown","metadata":{"id":"umxCML9BHeXY"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"phEDM5ADETYJ"},"outputs":[],"source":["plt.figure(figsize=(5,15))\n","import matplotlib.pyplot as plt\n","\n","# Extract loss history from the training history\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# Extract performance metrics from the results DataFrame\n","metrics = results_df['Metric']\n","values = results_df['Value']\n","\n","# Plotting\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n","\n","# Plot training and validation loss\n","axes[0].plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss', color='blue')\n","axes[0].plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss', color='orange')\n","axes[0].set_title('Training and Validation Loss')\n","axes[0].set_xlabel('Epochs')\n","axes[0].set_ylabel('Loss')\n","axes[0].legend()\n","\n","# Plot evaluation metrics\n","axes[1].bar(metrics, values, color=['blue', 'blue', 'blue', 'blue'])\n","axes[1].set_title('Evaluation Metrics')\n","axes[1].set_xlabel('Metric')\n","axes[1].set_ylabel('Value')\n","axes[1].set_ylim(0, 1)  # Setting y-axis limit to better visualize AUC\n","axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"kLBhitqRQbT4"},"outputs":[],"source":["from sklearn.metrics import cohen_kappa_score\n","import pandas as pd\n","\n","# Assuming y_true and y_pred are defined elsewhere in your code\n","# Calculate the Kappa score between the predictions and the actual data\n","kappa = cohen_kappa_score(y_true, y_pred)\n","\n","# Define the levels of kappa\n","kappa_levels = {\n","    'Excellent': (0.81, 1.0),\n","    'Very Good': (0.61, 0.8),\n","    'Good': (0.41, 0.6),\n","    'Fair': (0.21, 0.409),\n","    'Poor': (0.0, 0.2)\n","}\n","\n","# Determine the level of the kappa score\n","level = None\n","for category, (lower, upper) in kappa_levels.items():\n","    if lower <= kappa <= upper:\n","        level = category\n","        break\n","\n","# Create a DataFrame to display the kappa score and its level\n","results = pd.DataFrame({\n","    \"Metric\": [\"Kappa Score\"],\n","    \"Value\": [kappa],\n","    \"Level\": [level]\n","})\n","\n","# Print the results using tabulate\n","from tabulate import tabulate\n","print(tabulate(results, headers=\"keys\", tablefmt=\"pretty\"))\n"]},{"cell_type":"markdown","metadata":{"id":"DbVr56HeF7AM"},"source":[" The levels of results that can be obtained, they are as follows:\n","\n","    Excellent: If the Kappa score value is between 0.81 and 1.0.\n","    Very Good: If the Kappa score value is between 0.61 and 0.8.\n","    Good: If the Kappa score value is between 0.41 and 0.6.\n","    Fair: If the Kappa score value is between 0.21 and 0.4.\n","    Poor: If the Kappa score value is between 0.0 and 0.2.\n","\n","These levels assess the degree of agreement between predictions and actual data, where higher Kappa values indicate better agreement between data and predictions.\n"]},{"cell_type":"markdown","metadata":{"id":"z9sAzB_pLmbK"},"source":["## 7-2. Biases\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_mu8c5-LmbL"},"outputs":[],"source":["# Aequitas\n","from aequitas.preprocessing import preprocess_input_df\n","from aequitas.group import Group\n","from aequitas.plotting import Plot\n","from aequitas.bias import Bias\n","from aequitas.fairness import Fairness\n","\n","\n","ae_subset_df = pred_test_df[['race', 'gender', 'score', 'label_value']]\n","ae_df, _ = preprocess_input_df(ae_subset_df)\n","\n","g = Group()\n","xtab, _ = g.get_crosstabs(ae_df)\n","absolute_metrics = g.list_absolute_metrics(xtab)\n","clean_xtab = xtab.fillna(-1)\n","\n","aqp = Plot()\n","b = Bias()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvxsjAGcLmbM"},"outputs":[],"source":["# test reference group with Caucasian Male\n","bdf = b.get_disparity_predefined_groups(clean_xtab,\n","                                        original_df=ae_df,\n","                                        ref_groups_dict={'race':'Caucasian', 'gender':'Male'},\n","                                        alpha=0.05,\n","                                        check_significance=False)\n","\n","f = Fairness()\n","fdf = f.get_group_value_fairness(bdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"QEy_A5y8LmbN"},"outputs":[],"source":["# fairness: false positive rate\n","fpr_fairness = aqp.plot_fairness_group(fdf, group_metric='fpr', title=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"gEcesrlDE8zz"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fpr_fairness = aqp.plot_fairness_group(fdf, group_metric='fpr', title=True)\n","\n","#\n","plt.xticks(rotation=90)\n","\n","plt.show()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"tNDIlN76LmbO","scrolled":true},"outputs":[],"source":["# fairness: false negative rate\n","fnr_fairness = aqp.plot_fairness_group(fdf, group_metric='fnr', title=True)"]},{"cell_type":"markdown","metadata":{"id":"-ynLFtoXLmbP"},"source":["## 7-3. Disparity\n","\n","From the figures below we see that compared to Caucasian (the reference group), all other races have a higher false positive rate.\n","\n","On the other hand, we don't see this in gender. Gender doesn't not have much influence on the false positive rate."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"uqmUPi2NLmbQ"},"outputs":[],"source":["# FPR disparity of race\n","fpr_disparity = aqp.plot_disparity(bdf, group_metric='fpr_disparity', attribute_name='race')"]},{"cell_type":"markdown","metadata":{"id":"q3aQjVYPjrI5"},"source":["If the false positive rate (FPR) is lower, it means the model is better at classifying negative samples correctly as negatives, indicating better performance in predicting negative outcomes. Therefore, if the value is lower (such as 0.87), it signifies better performance compared to a higher value (like 1.96), hence the value 0.87 is better."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nWlPnGpnLmbR"},"outputs":[],"source":["# FPR disparity of gender\n","fpr_disparity = aqp.plot_disparity(bdf, group_metric='fpr_disparity', attribute_name='gender')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["iumRYRhALmZ1","wtsVQqniLma_","AlcQljT8LmbH"],"gpuType":"T4","provenance":[{"file_id":"1XCVbf8sW89evai7xRG-KaSQ7hXiK_lTR","timestamp":1734675147020},{"file_id":"14GpnZoD47EYbjMqJdrtSqPjq6BT2Zn2V","timestamp":1707555813433},{"file_id":"1qKo_xSfC0JzZIQs1izkmwqgpTjbSpMTc","timestamp":1707476777392}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}